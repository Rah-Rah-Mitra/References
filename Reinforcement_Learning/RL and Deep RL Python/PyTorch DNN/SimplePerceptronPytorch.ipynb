{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xSum(X,W):\n",
    "    h = torch.from_numpy(X)\n",
    "    z = torch.matmul(W,h) #matrics multiplication\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (1000,), array([0, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdim = 10 #features/columns\n",
    "n = 1000 #datapoints/rows\n",
    "X = np.random.rand(n,inputdim) \n",
    "y = np.random.randint(0,2,n) #between 0-1\n",
    "\n",
    "X.shape,y.shape, np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0027, 0.0763, 0.7127, 0.4023, 0.9599, 0.2864, 0.2187, 0.2537, 0.8438,\n",
       "        0.7549], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.tensor(np.random.uniform(0,1,inputdim),requires_grad=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1511, dtype=torch.float64, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = xSum(X[0,:],W)\n",
    "z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create a Neural Network with 1 input , 2 layers (1st with 2 neurons and 2nd with 3 neurons), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardStep(X,W_list):\n",
    "    h = torch.from_numpy(X) #outputs of computational neurons\n",
    "    for W in W_list:\n",
    "        h = torch.matmul(W,h) #passing the outputs for each neuron into next layer W\n",
    "    return h #Output h (input for next Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0075, dtype=torch.float64, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdim = 10 #features/columns\n",
    "n = 1000 #datapoints/rows\n",
    "X = np.random.rand(n,inputdim) \n",
    "y = np.random.randint(0,2,n) #between 0-1\n",
    "\n",
    "#Layers\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputdim)),requires_grad=True) #Layer 1 receiving input from X_train #2 in (2,inputdim) represents 2 neurons\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True) #Layer 2 receving input from Layer 1 #3 neurons receiving from 2 neurons so inputdim is 2\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True) #output receving input from Layer 2 #single binary classification output receving from 3 neurons\n",
    "W_list = [W1\n",
    "         ,W2\n",
    "         ,W3]\n",
    "\n",
    "#Feeding data through layers and generate output z\n",
    "z = forwardStep(X[0,:],W_list)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7849500775337219\n"
     ]
    }
   ],
   "source": [
    "activation_fun = nn.Sigmoid()\n",
    "# activation_fun = nn.ReLU()\n",
    "x = torch.randn(1) #input\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float) #True Label\n",
    "y_hat = activation_fun(x) #Predicted Label\n",
    "loss_fun = nn.BCELoss() #Binary Cross Entropy Loss\n",
    "                        #loss is a measure of performance\n",
    "                        #lower loss means better model\n",
    "loss_value = loss_fun(y_hat,y)\n",
    "print(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([0.4561]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss_fun = nn.BCELoss()\n",
    "lr = 0.0001\n",
    "x = torch.randn(1) #SINGLE INPUT\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float) #NO NEED INPUT DIMENSIONS\n",
    "w = torch.randn(1,requires_grad=True) #random weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855022311210632\n",
      "0.6855022311210632\n",
      "0.6855022311210632\n",
      "0.6855021715164185\n",
      "0.6855020523071289\n",
      "0.6855020523071289\n",
      "0.6855019330978394\n",
      "0.6855018138885498\n",
      "0.6855018138885498\n",
      "0.6855018138885498\n",
      "0.6855016946792603\n",
      "0.6855015754699707\n",
      "0.6855015754699707\n",
      "0.6855015754699707\n",
      "0.6855014562606812\n",
      "0.6855013370513916\n",
      "0.6855013370513916\n",
      "0.685501217842102\n",
      "0.6855010986328125\n",
      "0.6855010986328125\n",
      "0.6855010986328125\n",
      "0.685500979423523\n",
      "0.6855008602142334\n",
      "0.6855008602142334\n",
      "0.6855008602142334\n",
      "0.6855007410049438\n",
      "0.6855006217956543\n",
      "0.6855006217956543\n",
      "0.6855005025863647\n",
      "0.6855003833770752\n",
      "0.6855003833770752\n",
      "0.6855003833770752\n",
      "0.6855002641677856\n",
      "0.6855001449584961\n",
      "0.6855001449584961\n",
      "0.6855001449584961\n",
      "0.6855000257492065\n",
      "0.685499906539917\n",
      "0.685499906539917\n",
      "0.6854997873306274\n",
      "0.6854996681213379\n",
      "0.6854996681213379\n",
      "0.6854996681213379\n",
      "0.6854995489120483\n",
      "0.6854994297027588\n",
      "0.6854994297027588\n",
      "0.6854994297027588\n",
      "0.6854993104934692\n",
      "0.6854992508888245\n",
      "0.6854992508888245\n",
      "0.6854991316795349\n",
      "0.6854990124702454\n",
      "0.6854990124702454\n",
      "0.6854990124702454\n",
      "0.6854988932609558\n",
      "0.6854987740516663\n",
      "0.6854987740516663\n",
      "0.6854987740516663\n",
      "0.6854985356330872\n",
      "0.6854985356330872\n",
      "0.6854985356330872\n",
      "0.6854984164237976\n",
      "0.6854982972145081\n",
      "0.6854982972145081\n",
      "0.6854982972145081\n",
      "0.6854981780052185\n",
      "0.685498058795929\n",
      "0.685498058795929\n",
      "0.685498058795929\n",
      "0.6854978203773499\n",
      "0.6854978203773499\n",
      "0.6854978203773499\n",
      "0.6854977011680603\n",
      "0.6854975819587708\n",
      "0.6854975819587708\n",
      "0.6854975819587708\n",
      "0.6854974627494812\n",
      "0.6854973435401917\n",
      "0.6854973435401917\n",
      "0.6854972243309021\n",
      "0.6854971051216125\n",
      "0.6854971051216125\n",
      "0.6854971051216125\n",
      "0.685496985912323\n",
      "0.6854968667030334\n",
      "0.6854968667030334\n",
      "0.6854968667030334\n",
      "0.6854967474937439\n",
      "0.6854966282844543\n",
      "0.6854966282844543\n",
      "0.6854965090751648\n",
      "0.6854963898658752\n",
      "0.6854963898658752\n",
      "0.6854963898658752\n",
      "0.6854962706565857\n",
      "0.6854961514472961\n",
      "0.6854961514472961\n",
      "0.6854961514472961\n",
      "0.6854960322380066\n",
      "0.685495913028717\n"
     ]
    }
   ],
   "source": [
    "nIter = 100 #Gradient desent for 100 steps\n",
    "for i in range(nIter):\n",
    "    y_hat = m(w*x) #pass the input and weight through sigmoid m\n",
    "    loss = loss_fun(y_hat,y) #Calculate loss\n",
    "    loss.backward() #get Gradient for loss function with respect to W\n",
    "    dw = w.grad.data #differential w\n",
    "    with torch.no_grad():\n",
    "        w -= lr*dw #Gradient Descent\n",
    "    w.grad.data.zero_() #reset gradient data\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Pseudo Code For a Raw Neural Network\n",
    "#  _______  \n",
    "# |_______>  For nEpochs:\n",
    "# |.nEpoch    # We have a data set with Xn features and True Labels Y      \n",
    "# |\n",
    "# |  _____\n",
    "# | |_____> For nItern:\n",
    "# | | .nItern # Pick a random row from the data set (X1,X2 ... Xn) and its Corresponding Label Y \n",
    "# | |\n",
    "# | |\n",
    "# | |   # Input-Data (X1,X2 ... Xn) to First-Layer\n",
    "# | |        # --> First Layer (Add Weights AND Bias) = W0,X1W1,..XNWN  (in single neuron) (Only first Iteration use updated weights from second)\n",
    "# | |        # --> Weighted Sum (Z) =  W0 + X1W1 + X2W2 ... + XnWn  (in single neuron)\n",
    "# | |        # --> Pass through First Layer Activiation Function ( A_F() ) (in single neuron)\n",
    "# | |        # --> First Layer Output A_F(Z) (from single neuron)\n",
    "# | |        # --> Output -->> From all neurons in First Layer (A_F(Z)1,A_F(Z)2 ... A_F(Z)n)\n",
    "# | |    \n",
    "# | |    \n",
    "# | |    # First-Layer Output Data (A_F(Z)1,A_F(Z)2 ... A_F(Z)n) to Second-Layer\n",
    "# | |        # --> Second Layer (Add Weights AND Bias) = W0,W1(A_F(Z)1),W2(A_F(Z)2) ... Wn(A_F(Z)n) (Only first Iteration use updated weights from second) \n",
    "# | |        # --> Weighted Sum (Z) =  W0 + W1(A_F(Z)1) + W2(A_F(Z)2) ... Wn(A_F(Z)n)   \n",
    "# | |        # --> Second Layer Activiation Function ( A_F() )\n",
    "# | |        # --> Second Layer Output A_F(Z)\n",
    "# | |        # --> Output -->> From all neurons in Second Layer (A_F(Z)1,A_F(Z)2 ... A_F(Z)n)\n",
    "# | |    \n",
    "# | |    # ... (n-Dense Layers, Flatten Layers, Dropout Layers, Pooling Layers, BatchNormalisation Layers) ...\n",
    "# | |    \n",
    "# | |    \n",
    "# | |    # n-Layer Output Data (A_F(Z)1,A_F(Z)2 ... A_F(Z)n) to Output-Layer (Probability Label Layer)\n",
    "# | |        # --> Output Layer (Add Weights AND Bias) = W0,W1(A_F(Z)1),W2(A_F(Z)2) ... Wn(A_F(Z)n) (Only first Iteration use updated weights from second) \n",
    "# | |        # --> Weighted Sum (Z) =  W0 + W1(A_F(Z)1) + W2(A_F(Z)2) ... Wn(A_F(Z)n)   \n",
    "# | |        # --> Output Layer Activiation Function ( A_F() )\n",
    "# | |        # --> Output Layer Output A_F(Z) ~ y_hat , where y_hat is a probability distribution of possible output for data\n",
    "# | |    \n",
    "# | |    \n",
    "# | |    # Output-Layer (y_hat) to Loss Function to Compute Loss (Update Weights with new Weights from Gradient Descent)   _____\n",
    "# | |        # --> Loss() = Binary CrossEntropy Loss or MSLoss or CrossEntopy Loss                                         _____>  The placement of these steps \n",
    "# | |        # --> L(W) = Loss(y_hat, Y), where L(W) is basically a function made of Weights as Variables                  _____>  can influence the type of    \n",
    "# | |______  # --> Update Weight(W) for selected row to reduce Loss with Gradient Descent W -= (dL(W)/dW) * Learning rate  _____>  gradient descent: stochastic, \n",
    "# | |______< # --> End Iternation and start new Iteration with Updated Parameters W or End Epoch                           _____>  mini-batch or batch \n",
    "# | .Pick Rnd Row of Data\n",
    "# |\n",
    "# |_____________\n",
    "# |_____________> End Epoch Loop if Loss() tends to 0 as y_hat is close to actual Label Y or Epoch hits Max\n",
    "#  .Completed NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create out own sigmoid activation function\n",
    "def activate(x): #sigmoid activation\n",
    "    return 1/(1+torch.exp(-x)) #1/(1+e^-x)\n",
    "\n",
    "# Update Weights\n",
    "def updateParams(W_List,dW_List,lr):\n",
    "    with torch.no_grad(): # so we do not lose our initialised W's\n",
    "        for i in range(len(W_List)):\n",
    "            W_List[i] -= dW_List[i] * lr #update layer by layer\n",
    "    return W_List\n",
    "\n",
    "#Forward propagation between Layers\n",
    "def forwardStep(X,W_list):\n",
    "    h = torch.from_numpy(X) #outputs of computational neurons from previous layer\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h) #passing the outputs for each neuron into next layer W\n",
    "        h = activate(z) #Pass through Activation Function\n",
    "    return h #Output h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_sgd(X,y,W_list,loss_fun,lr=0.0001,nepochs=10):\n",
    "    for epoch in range(nepochs):\n",
    "        avgLoss = []\n",
    "        for i in range(len(y)): #go through each data row\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss = loss_fun(y_hat, torch.tensor(yTrue,dtype=torch.double)) #calculate loss\n",
    "            loss.backward() #Immediately compute gradient for current row of data\n",
    "            avgLoss.append(loss.item()) #record loss for each row\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data) #Create a List of W derivatives to update Old W \n",
    "            W_list = updateParams(W_list,dW_list,lr) #Immediately update W for current row of data \n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_() \n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,np.mean(np.array(avgLoss))))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_batch(X,y,W_list,loss_fun,lr=0.0001,nepochs=100):\n",
    "    n = len(y) # Complute datapoints outside\n",
    "    for epoch in range(nepochs):\n",
    "        loss = 0 # no need avg loss\n",
    "        for i in range(n):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss += loss_fun(y_hat,torch.tensor(yTrue,dtype=torch.double)) #Accumulate the loss\n",
    "        loss = loss/n #Avg Loss\n",
    "        loss.backward() #After loss is fully computed\n",
    "        sys.stdout.flush()\n",
    "        dW_list = []\n",
    "        for j in range(len(W_list)):\n",
    "            dW_list.append(W_list[j].grad.data)\n",
    "        W_list = updateParams(W_list,dW_list,lr)\n",
    "        for j in range(len(W_list)):\n",
    "            W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_minibatch(X,y,W_list,loss_fun,lr=0.0001,nepochs=100,batchSize=16):\n",
    "    n = len(y)\n",
    "    numBatches = n//batchSize\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch in range(numBatches):\n",
    "            X_batch = X[batch*batchSize:(batch+1)*batchSize,:]\n",
    "            y_batch = y[batch*batchSize:(batch+1)*batchSize]\n",
    "            loss = 0\n",
    "            for i in range(batchSize): #Vectorising this makes the code very fast\n",
    "                Xin = X_batch[i,:]\n",
    "                yTrue = y_batch[i]\n",
    "                y_hat = forwardStep(Xin,W_list)\n",
    "                loss += loss_fun(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss = loss/batchSize\n",
    "            loss.backward()\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss/numBatches))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch=0: 0.013781\n",
      "Loss after epoch=1: 0.013771\n",
      "Loss after epoch=2: 0.013760\n",
      "Loss after epoch=3: 0.013749\n",
      "Loss after epoch=4: 0.013738\n",
      "Loss after epoch=5: 0.013728\n",
      "Loss after epoch=6: 0.013717\n",
      "Loss after epoch=7: 0.013707\n",
      "Loss after epoch=8: 0.013696\n",
      "Loss after epoch=9: 0.013686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0702, 0.9476, 0.0570, 0.8654, 0.1617, 0.6987, 0.7447, 0.7541, 0.1051,\n",
       "          0.1382],\n",
       "         [0.0936, 0.2436, 0.5060, 0.6209, 0.0966, 0.2178, 0.5787, 0.0666, 0.6127,\n",
       "          0.1513]], dtype=torch.float64, requires_grad=True),\n",
       " tensor([[0.7735, 0.3378],\n",
       "         [0.8087, 0.7042],\n",
       "         [0.7176, 0.7092]], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.2918, 0.4962, 0.3904], dtype=torch.float64, requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Function\n",
    "loss_fun = nn.BCELoss()\n",
    "\n",
    "# Training Process\n",
    "inputdim = 10 #features/columns\n",
    "n = 1000 #datapoints/rows\n",
    "X = np.random.rand(n,inputdim) \n",
    "y = np.random.randint(0,2,n) #between 0-1\n",
    "\n",
    "#Layers\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputdim)),requires_grad=True) #Layer 1 receiving input from X_train #2 in (2,inputdim) represents 2 neurons\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True) #Layer 2 receving input from Layer 1 #3 neurons receiving from 2 neurons so inputdim is 2\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True) #output receving input from Layer 2 #single binary classification output receving from 3 neurons\n",
    "W_list = [W1\n",
    "         ,W2\n",
    "         ,W3]\n",
    "\n",
    "#Feeding data through layers and generate output    \n",
    "#   trainNN_sgd(X,y,W_list,loss_fun,lr=0.0001,nepochs=10) #Stochastic Gradient Descent\n",
    "#   trainNN_batch(X,y,W_list,loss_fun,lr=0.0001,nepochs=10)\n",
    "trainNN_minibatch(X,y,W_list,loss_fun,lr=0.0001,nepochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdim = 10 #features/columns\n",
    "n = 1000 #datapoints/rows\n",
    "X = np.random.rand(n,inputdim) \n",
    "y = np.random.randint(0,2,n) #between 0-1\n",
    "\n",
    "tensor_x = torch.Tensor(X) #Convert data to tensors\n",
    "tensor_y = torch.Tensor(y)\n",
    "\n",
    "Xy = TensorDataset(tensor_x,tensor_y) #Convert to Tensor Dataset\n",
    "Xy_loader = DataLoader(Xy, batch_size=16 ,shuffle=True, drop_last=True) #Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(inputdim,200,bias= True), #200 neurons layer 1\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(num_features=200), #input tensors are only 1 dimensional\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(200,100), #100 neurons layer 2\n",
    "    nn.Tanh(),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.Linear(100,1), #Output\n",
    "    nn.Sigmoid() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7120\n",
      "Epoch 2, Loss: 0.6475\n",
      "Epoch 3, Loss: 0.7701\n",
      "Epoch 4, Loss: 0.6521\n",
      "Epoch 5, Loss: 0.6975\n",
      "Epoch 6, Loss: 0.6352\n",
      "Epoch 7, Loss: 0.7184\n",
      "Epoch 8, Loss: 0.6658\n",
      "Epoch 9, Loss: 0.6794\n",
      "Epoch 10, Loss: 0.6842\n",
      "Epoch 11, Loss: 0.7240\n",
      "Epoch 12, Loss: 0.6431\n",
      "Epoch 13, Loss: 0.6594\n",
      "Epoch 14, Loss: 0.6839\n",
      "Epoch 15, Loss: 0.5784\n",
      "Epoch 16, Loss: 0.6848\n",
      "Epoch 17, Loss: 0.6680\n",
      "Epoch 18, Loss: 0.6412\n",
      "Epoch 19, Loss: 0.6549\n",
      "Epoch 20, Loss: 0.6244\n",
      "Epoch 21, Loss: 0.7171\n",
      "Epoch 22, Loss: 0.6141\n",
      "Epoch 23, Loss: 0.6160\n",
      "Epoch 24, Loss: 0.7380\n",
      "Epoch 25, Loss: 0.6994\n",
      "Epoch 26, Loss: 0.6879\n",
      "Epoch 27, Loss: 0.6866\n",
      "Epoch 28, Loss: 0.7166\n",
      "Epoch 29, Loss: 0.6023\n",
      "Epoch 30, Loss: 0.6484\n",
      "Epoch 31, Loss: 0.6968\n",
      "Epoch 32, Loss: 0.6907\n",
      "Epoch 33, Loss: 0.7046\n",
      "Epoch 34, Loss: 0.6380\n",
      "Epoch 35, Loss: 0.7254\n",
      "Epoch 36, Loss: 0.7008\n",
      "Epoch 37, Loss: 0.5858\n",
      "Epoch 38, Loss: 0.5722\n",
      "Epoch 39, Loss: 0.6216\n",
      "Epoch 40, Loss: 0.6311\n",
      "Epoch 41, Loss: 0.6334\n",
      "Epoch 42, Loss: 0.5960\n",
      "Epoch 43, Loss: 0.6024\n",
      "Epoch 44, Loss: 0.6146\n",
      "Epoch 45, Loss: 0.6069\n",
      "Epoch 46, Loss: 0.6580\n",
      "Epoch 47, Loss: 0.5275\n",
      "Epoch 48, Loss: 0.6008\n",
      "Epoch 49, Loss: 0.5663\n",
      "Epoch 50, Loss: 0.5674\n",
      "Epoch 51, Loss: 0.7057\n",
      "Epoch 52, Loss: 0.6484\n",
      "Epoch 53, Loss: 0.3970\n",
      "Epoch 54, Loss: 0.7218\n",
      "Epoch 55, Loss: 0.6669\n",
      "Epoch 56, Loss: 0.5619\n",
      "Epoch 57, Loss: 0.6631\n",
      "Epoch 58, Loss: 0.6452\n",
      "Epoch 59, Loss: 0.4983\n",
      "Epoch 60, Loss: 0.4739\n",
      "Epoch 61, Loss: 0.5669\n",
      "Epoch 62, Loss: 0.5323\n",
      "Epoch 63, Loss: 0.6429\n",
      "Epoch 64, Loss: 0.6753\n",
      "Epoch 65, Loss: 0.5274\n",
      "Epoch 66, Loss: 0.5125\n",
      "Epoch 67, Loss: 0.6698\n",
      "Epoch 68, Loss: 0.3700\n",
      "Epoch 69, Loss: 0.7004\n",
      "Epoch 70, Loss: 0.7408\n",
      "Epoch 71, Loss: 0.6476\n",
      "Epoch 72, Loss: 0.6068\n",
      "Epoch 73, Loss: 0.5187\n",
      "Epoch 74, Loss: 0.4492\n",
      "Epoch 75, Loss: 0.5563\n",
      "Epoch 76, Loss: 0.6404\n",
      "Epoch 77, Loss: 0.6812\n",
      "Epoch 78, Loss: 0.5332\n",
      "Epoch 79, Loss: 0.4807\n",
      "Epoch 80, Loss: 0.7397\n",
      "Epoch 81, Loss: 0.5950\n",
      "Epoch 82, Loss: 0.5533\n",
      "Epoch 83, Loss: 0.5045\n",
      "Epoch 84, Loss: 0.6464\n",
      "Epoch 85, Loss: 0.8661\n",
      "Epoch 86, Loss: 0.5500\n",
      "Epoch 87, Loss: 0.5744\n",
      "Epoch 88, Loss: 0.3718\n",
      "Epoch 89, Loss: 0.4732\n",
      "Epoch 90, Loss: 0.4357\n",
      "Epoch 91, Loss: 0.4047\n",
      "Epoch 92, Loss: 0.5711\n",
      "Epoch 93, Loss: 0.3954\n",
      "Epoch 94, Loss: 0.4057\n",
      "Epoch 95, Loss: 0.5645\n",
      "Epoch 96, Loss: 0.6889\n",
      "Epoch 97, Loss: 0.5096\n",
      "Epoch 98, Loss: 0.4097\n",
      "Epoch 99, Loss: 0.6050\n",
      "Epoch 100, Loss: 0.6053\n"
     ]
    }
   ],
   "source": [
    "nepochs = 100\n",
    "for epoch in range(nepochs):\n",
    "    for X, y in Xy_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        X = X.view(batch_size, -1)\n",
    "        y = y.view(batch_size, -1) # Ensure that y has the same shape as y_hat\n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch {}, Loss: {:.4f}\".format(epoch+1, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97317445\n"
     ]
    }
   ],
   "source": [
    "# lets predict a value\n",
    "with torch.no_grad():\n",
    "    xt = torch.tensor(np.random.rand(1,inputdim))\n",
    "    y2 = model(xt.float())\n",
    "    print(y2.detach().numpy()[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
