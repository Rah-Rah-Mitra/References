{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO #RL Algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 22.0\n",
      "Episode: 2 Score: 17.0\n",
      "Episode: 3 Score: 10.0\n",
      "Episode: 4 Score: 16.0\n",
      "Episode: 5 Score: 16.0\n",
      "Episode: 6 Score: 48.0\n",
      "Episode: 7 Score: 13.0\n",
      "Episode: 8 Score: 16.0\n",
      "Episode: 9 Score: 14.0\n",
      "Episode: 10 Score: 43.0\n",
      "Episode: 11 Score: 26.0\n",
      "Episode: 12 Score: 11.0\n",
      "Episode: 13 Score: 17.0\n",
      "Episode: 14 Score: 16.0\n",
      "Episode: 15 Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "# Take random Actions\n",
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample() #random action no training\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "    print(\"Episode: {} Score: {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2) 0\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "[-3.8892386e+00  1.6008305e+38  2.2484450e-02 -3.3747299e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space, #2 actions in discrete\n",
    "    env.action_space.sample()) #Integer Number\n",
    "\n",
    "print(env.observation_space) #Box\n",
    "print(env.observation_space.sample())\n",
    "\n",
    "#our algorithm needs to be able to function in both box and descrete envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda: env]) #Vectorised Environment, train on multiple environments\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='cuda') #Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1638 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1022        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009154532 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00617    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.55        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 49.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 951         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007356285 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0839      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 44          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 907          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076193167 |\n",
      "|    clip_fraction        | 0.0799       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.636       |\n",
      "|    explained_variance   | 0.206        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.9         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0192      |\n",
      "|    value_loss           | 55.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 893         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011358047 |\n",
      "|    clip_fraction        | 0.0756      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074640783 |\n",
      "|    clip_fraction        | 0.086        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.599       |\n",
      "|    explained_variance   | 0.387        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35           |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 61.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069051124 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.537        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.35         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    value_loss           | 49.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006324346 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00856    |\n",
      "|    value_loss           | 67.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052480083 |\n",
      "|    clip_fraction        | 0.0564       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | 0.166        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.2          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00821     |\n",
      "|    value_loss           | 49.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 890          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030933986 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | 0.262        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.824        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 15.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7bb8073a30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000) #Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model,env,n_eval_episodes= 10, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: [200.]\n",
      "Episode: 2 Score: [79.]\n",
      "Episode: 3 Score: [197.]\n",
      "Episode: 4 Score: [200.]\n",
      "Episode: 5 Score: [200.]\n"
     ]
    }
   ],
   "source": [
    "# Take random Actions\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(state) #random action no training\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "    print(\"Episode: {} Score: {}\".format(episode, score))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets implement some callbakcs and early stopping so that we are able to  stop if goal is reached\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"C:\\Users\\Rahul Mitra\\Desktop\\Python Projects\\vscode\\References\\Reinforcement Learning\\RL and Deep RL Python\\Deep Q-Learning\\StableBaseline Cart-Pole\\best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold = 196, verbose = 1)\n",
    "\n",
    "eval_callback = EvalCallback(env, callback_on_new_best=stop_callback, eval_freq=10000,\n",
    "                              best_model_save_path= save_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1) #Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 897  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093254605 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.685       |\n",
      "|    explained_variance   | -0.000978    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.59         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.019       |\n",
      "|    value_loss           | 49.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009369058 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.0792      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 37.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008816337 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 49.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul Mitra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=188.20 +/- 23.60\n",
      "Episode length: 188.20 +/- 23.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 188          |\n",
      "|    mean_reward          | 188          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072951093 |\n",
      "|    clip_fraction        | 0.0607       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | 0.26         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0145      |\n",
      "|    value_loss           | 62.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 595   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 586          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074545057 |\n",
      "|    clip_fraction        | 0.0879       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.584       |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.8         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 53.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 583          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069023063 |\n",
      "|    clip_fraction        | 0.0954       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.592       |\n",
      "|    explained_variance   | 0.698        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.016       |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 583         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007633227 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.43        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    value_loss           | 49.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004522441 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.837       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00965    |\n",
      "|    value_loss           | 19.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006417197 |\n",
      "|    clip_fraction        | 0.0537      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00653    |\n",
      "|    value_loss           | 26.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 200.00  is above the threshold 196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1eff510b610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback= eval_callback) #Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
