{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up an AlexNet in Keras. This is a relatively early network design, but goes quite deep compared to a multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the CIFAR images, normalize the images on all color channels 0-1, and one hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 10 # 10 output digits\n",
    "batch_size = 128 # mini batch\n",
    "epochs = 10 # total training loops\n",
    "learning_rate = 0.01 # amount we update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images = np.expand_dims(train_images / np.max(train_images), -1)\n",
    "test_images = np.expand_dims(test_images / np.max(test_images), -1)\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_outputs)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for our actual model. AlexNet was one of the original deep models that led to the resurgence of neural network techniques in machine learning. It had several innovations -- it was quite deep -- and it combined convolution with attenation -- merging together pixes with convolution, but adding more neural network depth.\n",
    "\n",
    "We'll create these networks without special techniques like dropout or batch normalization to get a simplified view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [11, 5, 3, 3, 3]\n",
    "filters = [96, 192, 384, 384, 256]\n",
    "pooling = [3, 3, 0, 0, 3]\n",
    "strides = [2, 2, 0, 0, 2]\n",
    "dense_units = [4096, 4096]\n",
    "image_shape = train_images.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a few loops to stack up the deep layers. This lets you get a sense that making a deep network is just about layering.\n",
    "\n",
    "We'll put in one placeholder layer to contain the image shape extracted frome the training data.\n",
    "\n",
    "Note the use of padding. This actually will pad the images. We need to do this here so that the input image is in fact big enough to 'divide' this many times. You'll see we in the final convolution we end up with a very small x and y dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_4 (Reshape)          (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 22, 22, 96)        34944     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 192)       460992    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 15, 15, 192)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 5, 5, 384)         663936    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 7, 7, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 5, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 7, 7, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 5, 5, 256)         884992    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              9441280   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 29,635,914\n",
      "Trainable params: 29,635,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "alexnet = Sequential()\n",
    "alexnet.add(Reshape(image_shape[:-1], input_shape=image_shape))\n",
    "for kernel, filter, pool, stride in zip(kernels, filters, pooling, strides):\n",
    "    alexnet.add(Conv2D(filter, kernel, activation='relu'))\n",
    "    alexnet.add(ZeroPadding2D(kernel//2))\n",
    "    if pool:\n",
    "        alexnet.add(MaxPooling2D(pool, strides=stride))\n",
    "\n",
    "alexnet.add(Flatten())\n",
    "\n",
    "for units in dense_units:\n",
    "    alexnet.add(Dense(units, activation='relu'))\n",
    "\n",
    "alexnet.add(Dense(num_outputs, activation='softmax'))\n",
    "alexnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as always, learning is done with an optimizer and a loss function, learning a classifier with categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "loss = keras.losses.categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, keep in mind this is starting to be a pretty big model. If you train this on a CPU, it is *possible*, but it is going to take a very long time. I'm running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 2.2673 - acc: 0.1658 - val_loss: 2.1837 - val_acc: 0.1784\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 2.0360 - acc: 0.2491 - val_loss: 1.9281 - val_acc: 0.2838\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 1.8752 - acc: 0.3144 - val_loss: 1.8306 - val_acc: 0.3188\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 1.7429 - acc: 0.3670 - val_loss: 1.6827 - val_acc: 0.3852\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 1.6454 - acc: 0.4036 - val_loss: 1.6272 - val_acc: 0.4042\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 1.5563 - acc: 0.4376 - val_loss: 1.4948 - val_acc: 0.4541\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 1.4900 - acc: 0.4631 - val_loss: 1.5237 - val_acc: 0.4518\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 1.4403 - acc: 0.4811 - val_loss: 1.5413 - val_acc: 0.4390\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 1.3947 - acc: 0.5012 - val_loss: 1.4221 - val_acc: 0.4883\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 1.3485 - acc: 0.5193 - val_loss: 1.5219 - val_acc: 0.4529\n"
     ]
    }
   ],
   "source": [
    "alexnet.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = alexnet.fit(train_images, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this isn't mind boggling accurate, but it is a very complex problem to recognize open images. We can see this model kept learning on each epoch, and didn't appear to overfit. You should as an experiment, increase the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
