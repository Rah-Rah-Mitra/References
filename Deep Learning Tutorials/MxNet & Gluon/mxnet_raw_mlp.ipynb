{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is a multi-layer-perceptron, in all its glory in a very raw MxNet format. Presented this way, it's very difficult to fully avoid the math, though you can certainly treat it as a cookbook recipe. Showing this raw format is mostly to demonstrate the utility of working with a higher level framework like Gluon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#This code is pure math and show how to develope a ML NN Model with this\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here controlling the size of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 28 * 28 # MNIST image size\n",
    "num_hidden = 256 # hidden layers\n",
    "num_outputs = 10 # 10 output digits\n",
    "batch_size = 64 # mini batch\n",
    "epochs = 10 # total training loops\n",
    "learning_rate = 0.001 # amount we update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started with our friends the MNIST digits. Data load, traditional normalization on [0-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " #building out the raw layers \n",
    "W1 = nd.random_normal(shape=(num_inputs, num_hidden)) #input to hidden\n",
    "b1 = nd.random_normal(shape=num_hidden)\n",
    "\n",
    "\n",
    "W2 = nd.random_normal(shape=(num_hidden, num_hidden)) #hidden to hidden\n",
    "b2 = nd.random_normal(shape=num_hidden)\n",
    "\n",
    "\n",
    "W3 = nd.random_normal(shape=(num_hidden, num_outputs)) #hidden to output\n",
    "b3 = nd.random_normal(shape=num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3] #input to hidden to output\n",
    "for param in params: #flow of parameters\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[4.3125005]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.max() #max value from all the pre initialised arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like the images -- we want to make sure out values range 0-1. We'll use an in place update with the `[:]` slice. This is saying -- hey assign all the individual number values to be these number values from the right hand side.\n",
    "\n",
    "This is one of those things -- that if you forget to do -- scale your number on the range [0-1] -- chances are you won't run get a model that works. This is about the learning rate -- if you have millions of numbers that range say [0-1000], but you are only updating 0.001 each loop -- it's just plain going to take *forever*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in params:\n",
    "    param[:] = param / param.max()\n",
    "W1.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to run over our layers -- we'll use the `relu`. As math goes, this one is pretty simple, it's 0 if we are less than zero. Basically just pulls through positive values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is out network, as a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def net(X):\n",
    "    # layer 1\n",
    "    h1_linear = nd.dot(X, W1) + b1\n",
    "    h1 = relu(h1_linear)\n",
    "\n",
    "    # layer 2\n",
    "    h2_linear = nd.dot(h1, W2) + b2\n",
    "    h2 = relu(h2_linear)\n",
    "\n",
    "    # output layer -- softmax will be computed as a loss in the training loop\n",
    "    yhat_linear = nd.dot(h2, W3) + b3\n",
    "    return yhat_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function and trainer! With raw MxNet -- we're left to ourselves to create our own stochastic gradient descent. Good news is -- we already learned that it really is just subtraction and multiplication!\n",
    "\n",
    "And our own loss function. Whoo -- that's math-y. Just remember - y are the actual values, from real data -- yhat, those are the ones the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)\n",
    "\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an accuracy metric. Loss is interesting to the model and the math, but understanding accuract as a percentage makes more sense to people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.reshape((-1, 784))\n",
    "        label = label\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 epoch loops, with a very small learning rate. This is basically doing all the work here in code, with one very important exception MxNet is computing the gradients for us -- the `autograd.record`. That's the real observation here, MxNet isn't a deep learning library so much as it is a symbolic math library with support for computing gradients built in.\n",
    "\n",
    "Same basic learning loop we have previously discussed, for each mini batch, run the network, while capturing the gradients and loss. Then update the parameters based on a learning function -- the optimizer -- and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.7073212, Train_acc 0.93986666, Test_acc 0.9323\n",
      "Epoch 1. Loss: 1.6286682, Train_acc 0.9636833, Test_acc 0.949\n",
      "Epoch 2. Loss: 1.0781705, Train_acc 0.9727833, Test_acc 0.955\n",
      "Epoch 3. Loss: 3.8051982, Train_acc 0.97728336, Test_acc 0.955\n",
      "Epoch 4. Loss: 3.9060752, Train_acc 0.9845, Test_acc 0.9606\n",
      "Epoch 5. Loss: 0.26946187, Train_acc 0.9859333, Test_acc 0.9602\n",
      "Epoch 6. Loss: 0.41502324, Train_acc 0.9892, Test_acc 0.963\n",
      "Epoch 7. Loss: 0.665453, Train_acc 0.99155, Test_acc 0.9634\n",
      "Epoch 8. Loss: 0.78512937, Train_acc 0.9943, Test_acc 0.9653\n",
      "Epoch 9. Loss: 0.34645534, Train_acc 0.99516666, Test_acc 0.9647\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.reshape((-1, 784))\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "    plt.imshow(learned_diagonal.asnumpy(), cmap='binary')\n",
    "    break\n",
    "\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, nd.sum(loss).asscalar(), train_accuracy, test_accuracy), \n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK -- that was fun, in a math-y way. Now let's try it developer style!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
