{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our final model, VGG16 in Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 10 # 10 output digits\n",
    "batch_size = 128 # mini batch\n",
    "epochs = 64 # total training loops\n",
    "learning_rate = 0.01 # amount we update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR images again. Note the need to transform so we are (color channel, x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    data = mx.nd.transpose(data, (2,0,1))\n",
    "    data = data.astype(np.float32) / 255.0\n",
    "    return data, label\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.CIFAR10(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.CIFAR10(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for the VGG blocks, 5 blocks in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [3, 3, 3, 3, 3]\n",
    "filters = [64, 128, 256, 512, 512]\n",
    "repeats = [2, 2, 3, 3, 3]\n",
    "pooling = [2, 2, 2, 2, 2]\n",
    "strides = [2, 2, 2, 2, 2]\n",
    "dense_units = [4096, 4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again --  you don't have a final softmax *layer*, MxNet handles he softmax inside this loss function, so the output is a straight linear mapping -- no activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = gluon.nn.HybridSequential()\n",
    "with vgg16.name_scope():\n",
    "    for kernel, filter, pool, stride, repeat in zip(kernels, filters, pooling, strides, repeats):\n",
    "        for _ in range(0, repeat): #looping and doing convolution and padding repeatedly for each layer\n",
    "            vgg16.add(gluon.nn.Conv2D(channels=filter, \n",
    "                                    kernel_size=kernel,\n",
    "                                    padding=(kernel//2),\n",
    "                                    activation='relu'))\n",
    "        vgg16.add(gluon.nn.MaxPool2D(pool_size=pool, \n",
    "                                       strides=stride,\n",
    "                                       padding=(kernel//2)))\n",
    "\n",
    "    vgg16.add(gluon.nn.Flatten())\n",
    "\n",
    "    for units in dense_units:\n",
    "        vgg16.add(gluon.nn.Dense(units, activation='relu'))\n",
    "    \n",
    "    vgg16.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluon requires a context, either `cpu` or `gpu`. You can change this to `cpu` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.collect_params().initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let't take a look at the resulting network. We need to feed in a sample batch to infer the network size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input                            (128, 3, 32, 32)               0\n",
      "        Activation-1   <Symbol hybridsequential0_conv0_relu_fwd>               0\n",
      "        Activation-2                           (128, 64, 32, 32)               0\n",
      "            Conv2D-3                           (128, 64, 32, 32)            1792\n",
      "        Activation-4   <Symbol hybridsequential0_conv1_relu_fwd>               0\n",
      "        Activation-5                           (128, 64, 32, 32)               0\n",
      "            Conv2D-6                           (128, 64, 32, 32)           36928\n",
      "         MaxPool2D-7                           (128, 64, 17, 17)               0\n",
      "        Activation-8   <Symbol hybridsequential0_conv2_relu_fwd>               0\n",
      "        Activation-9                          (128, 128, 17, 17)               0\n",
      "           Conv2D-10                          (128, 128, 17, 17)           73856\n",
      "       Activation-11   <Symbol hybridsequential0_conv3_relu_fwd>               0\n",
      "       Activation-12                          (128, 128, 17, 17)               0\n",
      "           Conv2D-13                          (128, 128, 17, 17)          147584\n",
      "        MaxPool2D-14                            (128, 128, 9, 9)               0\n",
      "       Activation-15   <Symbol hybridsequential0_conv4_relu_fwd>               0\n",
      "       Activation-16                            (128, 256, 9, 9)               0\n",
      "           Conv2D-17                            (128, 256, 9, 9)          295168\n",
      "       Activation-18   <Symbol hybridsequential0_conv5_relu_fwd>               0\n",
      "       Activation-19                            (128, 256, 9, 9)               0\n",
      "           Conv2D-20                            (128, 256, 9, 9)          590080\n",
      "       Activation-21   <Symbol hybridsequential0_conv6_relu_fwd>               0\n",
      "       Activation-22                            (128, 256, 9, 9)               0\n",
      "           Conv2D-23                            (128, 256, 9, 9)          590080\n",
      "        MaxPool2D-24                            (128, 256, 5, 5)               0\n",
      "       Activation-25   <Symbol hybridsequential0_conv7_relu_fwd>               0\n",
      "       Activation-26                            (128, 512, 5, 5)               0\n",
      "           Conv2D-27                            (128, 512, 5, 5)         1180160\n",
      "       Activation-28   <Symbol hybridsequential0_conv8_relu_fwd>               0\n",
      "       Activation-29                            (128, 512, 5, 5)               0\n",
      "           Conv2D-30                            (128, 512, 5, 5)         2359808\n",
      "       Activation-31   <Symbol hybridsequential0_conv9_relu_fwd>               0\n",
      "       Activation-32                            (128, 512, 5, 5)               0\n",
      "           Conv2D-33                            (128, 512, 5, 5)         2359808\n",
      "        MaxPool2D-34                            (128, 512, 3, 3)               0\n",
      "       Activation-35  <Symbol hybridsequential0_conv10_relu_fwd>               0\n",
      "       Activation-36                            (128, 512, 3, 3)               0\n",
      "           Conv2D-37                            (128, 512, 3, 3)         2359808\n",
      "       Activation-38  <Symbol hybridsequential0_conv11_relu_fwd>               0\n",
      "       Activation-39                            (128, 512, 3, 3)               0\n",
      "           Conv2D-40                            (128, 512, 3, 3)         2359808\n",
      "       Activation-41  <Symbol hybridsequential0_conv12_relu_fwd>               0\n",
      "       Activation-42                            (128, 512, 3, 3)               0\n",
      "           Conv2D-43                            (128, 512, 3, 3)         2359808\n",
      "        MaxPool2D-44                            (128, 512, 2, 2)               0\n",
      "          Flatten-45                                 (128, 2048)               0\n",
      "       Activation-46  <Symbol hybridsequential0_dense0_relu_fwd>               0\n",
      "       Activation-47                                 (128, 4096)               0\n",
      "            Dense-48                                 (128, 4096)         8392704\n",
      "       Activation-49  <Symbol hybridsequential0_dense1_relu_fwd>               0\n",
      "       Activation-50                                 (128, 4096)               0\n",
      "            Dense-51                                 (128, 4096)        16781312\n",
      "            Dense-52                                   (128, 10)           40970\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 39929674\n",
      "   Trainable params: 39929674\n",
      "   Non-trainable params: 0\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 39929674\n",
      "--------------------------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i, (d, l) in enumerate(train_data):\n",
    "    print(vgg16.summary(d.as_in_context(ctx)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you network doesn't change shape, you can `hybridize` it, which makes Gluon run in a precomplied mode much like Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as always, learning is done with an optimizer and a loss function, learning a classifier with categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(vgg16.collect_params(), 'sgd', {'learning_rate': learning_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.3023834320806125, Train_acc 0.11012, Test_acc 0.1092, Time 25.231127\n",
      "Epoch 1. Loss: 2.302035459340422, Train_acc 0.12996, Test_acc 0.1306, Time 24.935430\n",
      "Epoch 2. Loss: 2.3014975379081557, Train_acc 0.18354, Test_acc 0.1889, Time 24.405735\n",
      "Epoch 3. Loss: 2.3002735904957174, Train_acc 0.18038, Test_acc 0.1811, Time 22.922612\n",
      "Epoch 4. Loss: 2.2966594224527874, Train_acc 0.2035, Test_acc 0.2073, Time 22.926140\n",
      "Epoch 5. Loss: 2.2814693055800324, Train_acc 0.19408, Test_acc 0.1907, Time 23.609636\n",
      "Epoch 6. Loss: 2.1740437756168203, Train_acc 0.19998, Test_acc 0.2026, Time 22.846272\n",
      "Epoch 7. Loss: 2.063757028394381, Train_acc 0.23552, Test_acc 0.2366, Time 25.318111\n",
      "Epoch 8. Loss: 2.01008239545407, Train_acc 0.26302, Test_acc 0.2725, Time 22.835264\n",
      "Epoch 9. Loss: 1.9519827310537334, Train_acc 0.29422, Test_acc 0.2997, Time 23.640966\n",
      "Epoch 10. Loss: 1.9220144043531193, Train_acc 0.28886, Test_acc 0.2941, Time 24.765907\n",
      "Epoch 11. Loss: 1.8752749971809894, Train_acc 0.3061, Test_acc 0.3101, Time 23.569700\n",
      "Epoch 12. Loss: 1.8230242061027557, Train_acc 0.33356, Test_acc 0.3371, Time 23.653461\n",
      "Epoch 13. Loss: 1.7673647803654629, Train_acc 0.34366, Test_acc 0.3556, Time 24.977016\n",
      "Epoch 14. Loss: 1.7385266678344653, Train_acc 0.36666, Test_acc 0.3796, Time 23.391392\n",
      "Epoch 15. Loss: 1.6923494319591126, Train_acc 0.39712, Test_acc 0.4033, Time 23.239229\n",
      "Epoch 16. Loss: 1.636476111360613, Train_acc 0.40932, Test_acc 0.4089, Time 23.598413\n",
      "Epoch 17. Loss: 1.591815090719809, Train_acc 0.44036, Test_acc 0.4387, Time 23.522786\n",
      "Epoch 18. Loss: 1.5572808013873527, Train_acc 0.44282, Test_acc 0.4393, Time 23.256861\n",
      "Epoch 19. Loss: 1.506305521739616, Train_acc 0.45222, Test_acc 0.4489, Time 23.561821\n",
      "Epoch 20. Loss: 1.467878543246569, Train_acc 0.48762, Test_acc 0.4815, Time 23.561648\n",
      "Epoch 21. Loss: 1.4443103131363442, Train_acc 0.48864, Test_acc 0.4782, Time 23.338123\n",
      "Epoch 22. Loss: 1.4109909534507563, Train_acc 0.48978, Test_acc 0.4811, Time 24.718201\n",
      "Epoch 23. Loss: 1.3772011810044282, Train_acc 0.52252, Test_acc 0.5066, Time 22.774350\n",
      "Epoch 24. Loss: 1.352228013466337, Train_acc 0.52404, Test_acc 0.5089, Time 25.116296\n",
      "Epoch 25. Loss: 1.3071652977787598, Train_acc 0.53646, Test_acc 0.5138, Time 23.865644\n",
      "Epoch 26. Loss: 1.282219790204511, Train_acc 0.52858, Test_acc 0.5106, Time 23.227528\n",
      "Epoch 27. Loss: 1.2554501793182504, Train_acc 0.55836, Test_acc 0.5309, Time 23.719197\n",
      "Epoch 28. Loss: 1.2067305634419112, Train_acc 0.57724, Test_acc 0.5347, Time 23.458187\n",
      "Epoch 29. Loss: 1.1749449516450863, Train_acc 0.59594, Test_acc 0.5557, Time 23.679408\n",
      "Epoch 30. Loss: 1.1464541926294867, Train_acc 0.61234, Test_acc 0.5602, Time 23.546608\n",
      "Epoch 31. Loss: 1.1166168805314967, Train_acc 0.6206, Test_acc 0.5692, Time 23.665919\n",
      "Epoch 32. Loss: 1.073146281565713, Train_acc 0.61206, Test_acc 0.5517, Time 24.408704\n",
      "Epoch 33. Loss: 1.0367406726779396, Train_acc 0.6478, Test_acc 0.5696, Time 23.733989\n",
      "Epoch 34. Loss: 0.9872379022971397, Train_acc 0.65176, Test_acc 0.5733, Time 24.393414\n",
      "Epoch 35. Loss: 0.9438590826896949, Train_acc 0.68364, Test_acc 0.5917, Time 25.005989\n",
      "Epoch 36. Loss: 0.9045597995665459, Train_acc 0.70962, Test_acc 0.5927, Time 24.819955\n",
      "Epoch 37. Loss: 0.8653128792176159, Train_acc 0.73584, Test_acc 0.5975, Time 24.220595\n",
      "Epoch 38. Loss: 0.8197147546217847, Train_acc 0.76854, Test_acc 0.6049, Time 24.692624\n",
      "Epoch 39. Loss: 0.748002793758278, Train_acc 0.75088, Test_acc 0.5866, Time 23.853289\n",
      "Epoch 40. Loss: 0.7162088729695103, Train_acc 0.75262, Test_acc 0.578, Time 24.132281\n",
      "Epoch 41. Loss: 0.6618146404761348, Train_acc 0.7683, Test_acc 0.5756, Time 22.936145\n",
      "Epoch 42. Loss: 0.5869373975035644, Train_acc 0.81218, Test_acc 0.5865, Time 23.899590\n",
      "Epoch 43. Loss: 0.5322829033493444, Train_acc 0.7006, Test_acc 0.5174, Time 24.048471\n",
      "Epoch 44. Loss: 0.4713844113950151, Train_acc 0.75266, Test_acc 0.5518, Time 24.835320\n",
      "Epoch 45. Loss: 0.4007436782778915, Train_acc 0.93404, Test_acc 0.6201, Time 23.691268\n",
      "Epoch 46. Loss: 0.3524173216166068, Train_acc 0.87028, Test_acc 0.5827, Time 24.219633\n",
      "Epoch 47. Loss: 0.29545822604465793, Train_acc 0.94978, Test_acc 0.6107, Time 22.865353\n",
      "Epoch 48. Loss: 0.2594321194448693, Train_acc 0.94542, Test_acc 0.607, Time 22.967931\n",
      "Epoch 49. Loss: 0.20937273698213452, Train_acc 0.82212, Test_acc 0.5504, Time 22.769934\n",
      "Epoch 50. Loss: 0.1935978749168056, Train_acc 0.47852, Test_acc 0.3804, Time 23.885295\n",
      "Epoch 51. Loss: 0.1590892430755024, Train_acc 0.94894, Test_acc 0.5948, Time 22.848909\n",
      "Epoch 52. Loss: 0.1418673263983744, Train_acc 0.9694, Test_acc 0.6044, Time 23.210452\n",
      "Epoch 53. Loss: 0.15381894724685954, Train_acc 0.9873, Test_acc 0.6178, Time 24.001818\n",
      "Epoch 54. Loss: 0.12013144398854904, Train_acc 0.98418, Test_acc 0.6189, Time 23.964758\n",
      "Epoch 55. Loss: 0.07880710670183906, Train_acc 0.977, Test_acc 0.615, Time 24.500886\n",
      "Epoch 56. Loss: 0.06422180123433256, Train_acc 0.99406, Test_acc 0.6253, Time 23.584915\n",
      "Epoch 57. Loss: 0.04907928761089582, Train_acc 0.9734, Test_acc 0.609, Time 24.203238\n",
      "Epoch 58. Loss: 0.05929824623676096, Train_acc 0.99312, Test_acc 0.6257, Time 24.188332\n",
      "Epoch 59. Loss: 0.048941031540580764, Train_acc 0.9972, Test_acc 0.6277, Time 24.179772\n",
      "Epoch 60. Loss: 0.06500179997419517, Train_acc 0.98294, Test_acc 0.621, Time 24.199081\n",
      "Epoch 61. Loss: 0.036961820080603336, Train_acc 0.98862, Test_acc 0.6204, Time 24.145191\n",
      "Epoch 62. Loss: 0.02922856702749047, Train_acc 0.99374, Test_acc 0.6242, Time 25.519769\n",
      "Epoch 63. Loss: 0.03724061300212316, Train_acc 0.99282, Test_acc 0.6207, Time 24.852785\n"
     ]
    }
   ],
   "source": [
    "smoothing_constant = .01\n",
    "moving_loss = 0.0\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time()\n",
    "    for i, (d, l) in enumerate(train_data):\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = l.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = vgg16(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        #  Keep a moving average of the losses\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "    elapsed = time() - start\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, vgg16)\n",
    "    train_accuracy = evaluate_accuracy(train_data, vgg16)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s, Time %f\" % (e, moving_loss, train_accuracy, test_accuracy, elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this one a little longer, syou can see the training accuracy shoots way up -- but there is a big gap between the training and testing accuracy. This is what we call overfitting -- the model has in a sense memorized the inputs, making a very expensive neural network hashtable!\n",
    "\n",
    "In the next section we'll investigate techniques to overcome this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
