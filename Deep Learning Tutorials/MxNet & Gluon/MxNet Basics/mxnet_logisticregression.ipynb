{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had previously studied logistic regression, so let's revisit logistic regression building a network on top of gluon, on top of MxNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our good friends -- the MNIST digits, which are available in Gluon/MxNet. As a general rule, when you are playing with a new learning framework, the MNIST digits are -- well you can think of them as `Hello World`.\n",
    "\n",
    "Our transform function here is our general rule of turning the data on to the range of 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Familiar data structures here, batch of 32, with the 28x28 MNIST images with 1 color channel -- greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_data:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start making a network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Dense(10) #10 classes\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=1.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we apply our cookbook -- loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now -- metrics -- here is an accuracy definition. Here we are iterating over a set of data, running the network to compute the output value, and then using argmax to find the 0, 1 ... 8, 9 slot that has the largest value, and thus the highest probability. That's out prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the training loop. For a number of epochs -- the outer loop, we run the network recording gradients and computing the loss.\n",
    "With these recorded gradients, we run the loss backward -- this is the backward propagation that drives the learning.\n",
    "\n",
    "Notice for the softmax cross entropy, we didn't explicity need to conver to one-hot, MxNet can work with the ordinal integer class labels for out digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 24.067093, Train_acc 0.8371333333333333, Test_acc 0.8486\n",
      "Epoch 1. Loss: 26.175669, Train_acc 0.8666833333333334, Test_acc 0.875\n",
      "Epoch 2. Loss: 17.657513, Train_acc 0.8784166666666666, Test_acc 0.8853\n",
      "Epoch 3. Loss: 5.768581, Train_acc 0.8862666666666666, Test_acc 0.8921\n",
      "Epoch 4. Loss: 20.001282, Train_acc 0.8910333333333333, Test_acc 0.8934\n",
      "Epoch 5. Loss: 8.275906, Train_acc 0.8967666666666667, Test_acc 0.8992\n",
      "Epoch 6. Loss: 39.50508, Train_acc 0.89825, Test_acc 0.8967\n",
      "Epoch 7. Loss: 13.759548, Train_acc 0.9021666666666667, Test_acc 0.9032\n",
      "Epoch 8. Loss: 1.7785668, Train_acc 0.9047333333333333, Test_acc 0.9047\n",
      "Epoch 9. Loss: 5.6207404, Train_acc 0.90805, Test_acc 0.9055\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % \n",
    "          (e, nd.sum(loss).asscalar(), train_accuracy, test_accuracy), \n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's Gluon. We'll learn how to make more powerful networks in subsequent videos, but for now -- remember that we didn't really specify many array shapes -- we loaded up images, and declared that there are 10 output classes -- the 10 digits -- with a single `Dense` layer. This is one of the advantages of Gluon, shape inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
