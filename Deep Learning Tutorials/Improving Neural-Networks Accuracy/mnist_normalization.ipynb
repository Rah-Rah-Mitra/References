{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras, let's look at the effect of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Flatten, MaxPooling2D, MaxPooling1D, Conv2D, Reshape, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up MNIST digits. Explicitly *not* normalizing the pixels to the range 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train_not_normalized = np.expand_dims(x_train, -1)\n",
    "x_test_not_normalized = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the digit labels for the numbers 0...9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = keras.utils.to_categorical(y_train, 10)\n",
    "test_labels = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple convolutional network with pooling and a dense output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 14.5091 - acc: 0.0995 - val_loss: 14.4918 - val_acc: 0.1009\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 14.5200 - acc: 0.0992 - val_loss: 14.4918 - val_acc: 0.1009\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 14.5200 - acc: 0.0992 - val_loss: 14.4918 - val_acc: 0.1009\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 14.5200 - acc: 0.0992 - val_loss: 14.4918 - val_acc: 0.1009\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 14.5200 - acc: 0.0992 - val_loss: 14.4918 - val_acc: 0.1009\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 14.5200 - acc: 0.0992 - val_loss: 14.4918 - val_acc: 0.1009\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 14.5200 - acc: 0.0992 - val_loss: 14.4918 - val_acc: 0.1009\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 14.5200 - acc: 0.0992 - val_loss: 14.4918 - val_acc: 0.1009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d186ae438>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train_not_normalized[0].shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape(input_shape, input_shape=input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_not_normalized, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test_not_normalized, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK -- that didn't learn. The gradients 'exploded' -- went to very large values and wandered off. It also doesn't help that the learning rate by default is small enough that it cannot work well on parameter values outside of 0-1. This is an important point, and a bit of a cookbook recipe -- keep all your numbers in the range 0.1 to avoid trouble.\n",
    "\n",
    "Now -- Normalize the images by rescaling on 0-1, using the max image value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized = np.expand_dims(x_train / np.max(x_train), -1)\n",
    "x_test_normalized = np.expand_dims(x_test / np.max(x_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.1303 - acc: 0.9601 - val_loss: 0.0443 - val_acc: 0.9857\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0390 - acc: 0.9882 - val_loss: 0.0469 - val_acc: 0.9846\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0258 - acc: 0.9916 - val_loss: 0.0360 - val_acc: 0.9889\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.0171 - acc: 0.9945 - val_loss: 0.0339 - val_acc: 0.9900\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0143 - acc: 0.9951 - val_loss: 0.0373 - val_acc: 0.9894\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0306 - val_acc: 0.9904\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0300 - val_acc: 0.9909\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0458 - val_acc: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d006c4c18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train_normalized[0].shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape(input_shape, input_shape=input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_normalized, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test_normalized, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh -- back to decent results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now -- a slightly more advanced technique -- *batch normalization*. This will automatically normalize your data each batch, mapping the numbers down to a range of 0-1.\n",
    "\n",
    "We'll work on the raw pixel input to illustrate the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.1024 - acc: 0.9697 - val_loss: 0.0559 - val_acc: 0.9828\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0381 - acc: 0.9887 - val_loss: 0.0399 - val_acc: 0.9879\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0520 - val_acc: 0.9825\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0190 - acc: 0.9939 - val_loss: 0.0539 - val_acc: 0.9836\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0149 - acc: 0.9952 - val_loss: 0.0387 - val_acc: 0.9887\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0128 - acc: 0.9956 - val_loss: 0.0580 - val_acc: 0.9834\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.0114 - acc: 0.9964 - val_loss: 0.0356 - val_acc: 0.9903\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0424 - val_acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d18138d68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train_not_normalized[0].shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape(input_shape, input_shape=input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_not_normalized, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test_not_normalized, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's a pretty good layer. Even though we 'forgot' to normalize out input data -- we made out network self-normalizing as it runs. \n",
    "\n",
    "I tend to use this technique myself, as it avoids pre-processing of data which tends to be on the CPU and much slower, while this `BatchNormalization`, when I run it on the GPU for sure adds some runtime to the model -- notice the difference in seconds -- but is a lot more forgiving of your input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
