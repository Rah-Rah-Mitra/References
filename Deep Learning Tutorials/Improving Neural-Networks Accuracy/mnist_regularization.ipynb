{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras, let's look at the effect of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Flatten, MaxPooling2D, MaxPooling1D, Conv2D, Reshape, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the images by rescaling on 0-1, using the max image value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train / np.max(x_train), -1)\n",
    "x_test = np.expand_dims(x_test / np.max(x_test), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the digit labels for the numbers 0...9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = keras.utils.to_categorical(y_train, 10)\n",
    "test_labels = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple convolutional network with pooling and a dense output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.1350 - acc: 0.9590 - val_loss: 0.0490 - val_acc: 0.9843\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0401 - acc: 0.9877 - val_loss: 0.0437 - val_acc: 0.9864\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0252 - acc: 0.9919 - val_loss: 0.0339 - val_acc: 0.9893\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0181 - acc: 0.9942 - val_loss: 0.0448 - val_acc: 0.9879\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0136 - acc: 0.9954 - val_loss: 0.0369 - val_acc: 0.9880\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0463 - val_acc: 0.9866\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0393 - val_acc: 0.9906\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0382 - val_acc: 0.9892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1d353b400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train[0].shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape(input_shape, input_shape=input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now -- look at the gap between the acc and val_acc -- that's the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the same model but with L2 regularization. This applies a penalty function, you can think of it as a kind of additional loss. The benefit is -- less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.8421 - acc: 0.9156 - val_loss: 0.4289 - val_acc: 0.9555\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.3939 - acc: 0.9495 - val_loss: 0.3622 - val_acc: 0.9459\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.3274 - acc: 0.9569 - val_loss: 0.3043 - val_acc: 0.9574\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.3054 - acc: 0.9593 - val_loss: 0.2781 - val_acc: 0.9656\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.2888 - acc: 0.9609 - val_loss: 0.2684 - val_acc: 0.9645\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 8s 142us/step - loss: 0.2775 - acc: 0.9625 - val_loss: 0.2482 - val_acc: 0.9678\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.2707 - acc: 0.9615 - val_loss: 0.2355 - val_acc: 0.9708\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.2632 - acc: 0.9629 - val_loss: 0.2360 - val_acc: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1d04f4f60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_regularizer=regularizers.l2(0.01)\n",
    "\n",
    "regularized = Sequential()\n",
    "regularized.add(Reshape(input_shape, input_shape=input_shape))\n",
    "regularized.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "regularized.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "regularized.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "regularized.add(Flatten())\n",
    "regularized.add(Dense(128, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "regularized.add(Dense(128, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "regularized.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "regularized.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "regularized.fit(x_train, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the model is slower to learn, comparing the smaller initial accuracy and larger loss compared to the non-regularized prior run. This higher loss is simply less overfitting to the training data.\n",
    "\n",
    "And -- look at the result -- the `val_acc` is actually higher than the `acc`, which while less accurate -- has led our model to be more generalized, which is to say -- less memorized on the training data.\n",
    "\n",
    "This is an interestind tradeoff -- but can be overcome with more training data and more epochs of training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a different approach -- *Dropout*. This will randomly deactive individual parameters while training, but not while validating or predicting. The notion is that the network is forced to adapt to the data and generalize as it cannot rely on which parameters will be active in any given training run.\n",
    "\n",
    "We'll add a 50% dropout after each learning layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 1.2239 - acc: 0.8211 - val_loss: 0.5980 - val_acc: 0.9546\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.6569 - acc: 0.9033 - val_loss: 0.4828 - val_acc: 0.9607\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.5933 - acc: 0.9141 - val_loss: 0.4530 - val_acc: 0.9634\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.5658 - acc: 0.9168 - val_loss: 0.4420 - val_acc: 0.9631\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.5508 - acc: 0.9169 - val_loss: 0.4362 - val_acc: 0.9589\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.5367 - acc: 0.9206 - val_loss: 0.4136 - val_acc: 0.9645\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.5286 - acc: 0.9221 - val_loss: 0.3947 - val_acc: 0.9678\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.5263 - acc: 0.9204 - val_loss: 0.3900 - val_acc: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff12f7ee5f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dropped = Sequential()\n",
    "dropped.add(Reshape(input_shape, input_shape=input_shape))\n",
    "dropped.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.5))\n",
    "dropped.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.5))\n",
    "dropped.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "dropped.add(Flatten())\n",
    "dropped.add(Dense(128, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.5))\n",
    "dropped.add(Dense(128, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.5))\n",
    "dropped.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "dropped.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "dropped.fit(x_train, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And -- again, a slower learning rate, but better generalization. Dropout is a bit easier to understand than L2 regularization, which is a bit *math-y*, and interacts with the loss function and learning rate. It can be difficult to reason about parameter values.\n",
    "\n",
    "Dropout is simpler, you can vary the dropout effect by choosing different percentages, and more training loops. Here it is with 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.9120 - acc: 0.9033 - val_loss: 0.4685 - val_acc: 0.9569\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.4515 - acc: 0.9417 - val_loss: 0.3627 - val_acc: 0.9663\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.4030 - acc: 0.9481 - val_loss: 0.3515 - val_acc: 0.9619\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.3844 - acc: 0.9481 - val_loss: 0.3427 - val_acc: 0.9676\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3678 - acc: 0.9512 - val_loss: 0.3149 - val_acc: 0.9707\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.3555 - acc: 0.9519 - val_loss: 0.3182 - val_acc: 0.9634\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.3470 - acc: 0.9531 - val_loss: 0.3092 - val_acc: 0.9672\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.3412 - acc: 0.9527 - val_loss: 0.2958 - val_acc: 0.9687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee584bd320>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped = Sequential()\n",
    "dropped.add(Reshape(input_shape, input_shape=input_shape))\n",
    "dropped.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.2))\n",
    "dropped.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.2))\n",
    "dropped.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "dropped.add(Flatten())\n",
    "dropped.add(Dense(128, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.2))\n",
    "dropped.add(Dense(128, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "dropped.add(Dropout(0.2))\n",
    "dropped.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "dropped.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "dropped.fit(x_train, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty good compromise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
