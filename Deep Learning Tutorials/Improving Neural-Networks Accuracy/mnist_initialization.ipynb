{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the effect of different initialization on gradients. We'll use Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Flatten, MaxPooling2D, MaxPooling1D, Conv2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the images by rescaling on 0-1, using the max image value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train / np.max(x_train), -1)\n",
    "x_test = np.expand_dims(x_test / np.max(x_test), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the digit labels for the numbers 0...9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = keras.utils.to_categorical(y_train, 10)\n",
    "test_labels = keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple convolutional network with pooling and a dense output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "\n",
    "def build_network(initializer):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape(input_shape, input_shape=input_shape))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer = initializer))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer = initializer))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer = initializer))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer = initializer))\n",
    "    model.add(Dense(10, activation='softmax', kernel_initializer = initializer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've seen this kind of model work before, but let's change just one thing and initialize the learning with zeros.\n",
    "\n",
    "Keras has named initializers, so all we do is pass in the string `'zeros'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 83s 1ms/step - loss: 2.3016 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 83s 1ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n"
     ]
    }
   ],
   "source": [
    "model = build_network('zeros')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, train_labels,\n",
    "                    batch_size=64,\n",
    "                    epochs=8,\n",
    "                    validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Same network -- but we didn't learn anything. By setting all the initial values to zero, we've run into the *vanishing gradients* problem. Without diving into the math, the  simple way to think about it is that a parameter at 0 -- the gradient is 0 -- and the model doesn't understand if it should increase or decrease the parameter in order to learn. So it just gets stuck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing -- but now initialize to all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 83s 1ms/step - loss: 14.4629 - acc: 0.1021 - val_loss: 14.4902 - val_acc: 0.1010\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 14.4711 - acc: 0.1022 - val_loss: 14.4902 - val_acc: 0.1010\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 14.4711 - acc: 0.1022 - val_loss: 14.4902 - val_acc: 0.1010\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 2226s 37ms/step - loss: 14.4711 - acc: 0.1022 - val_loss: 14.4902 - val_acc: 0.1010\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 8376s 140ms/step - loss: 14.4711 - acc: 0.1022 - val_loss: 14.4902 - val_acc: 0.1010\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 14.4711 - acc: 0.1022 - val_loss: 14.4902 - val_acc: 0.1010\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 14.4711 - acc: 0.1022 - val_loss: 14.4902 - val_acc: 0.1010\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 14.4711 - acc: 0.1022 - val_loss: 14.4902 - val_acc: 0.1010\n"
     ]
    }
   ],
   "source": [
    "model = build_network('ones')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, train_labels,\n",
    "                    batch_size=64,\n",
    "                    epochs=8,\n",
    "                    validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again -- the network fails to learn. This is the *exploding gradients* problem, where the gradients at any parameter -- in this case all parameters -- are positive, the model can only increase the value of parameters by the learning rate. In this case the model isn't so much stuck, as it is wandeirng off in the wilderness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use a proper initialization schema -- Glorot Uniform -- which draws a random sample of small floating point numbers centered around zero. This will give us a mix of positive and negative parameters, with positive and negative gradients, allowing the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.1344 - acc: 0.9587 - val_loss: 0.0479 - val_acc: 0.9844\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0385 - acc: 0.9883 - val_loss: 0.0451 - val_acc: 0.9843\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0245 - acc: 0.9923 - val_loss: 0.0401 - val_acc: 0.9881\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0178 - acc: 0.9942 - val_loss: 0.0542 - val_acc: 0.9852\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0401 - val_acc: 0.9887\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0369 - val_acc: 0.9899\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0387 - val_acc: 0.9901\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0362 - val_acc: 0.9912\n"
     ]
    }
   ],
   "source": [
    "model = build_network('glorot_uniform')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, train_labels,\n",
    "                    batch_size=64,\n",
    "                    epochs=8,\n",
    "                    validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all we did was change the initialization! This is an important point to keep in mind, when we are doing stochastic methods, we need random initialization that has the ability to generate small numbers and not get 'stuck' at zero, or exploding off to very large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final initialization method to consider He Normal, is conceptually similar to Glorot Uniform, but generates slightly larger numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 0.1400 - acc: 0.9584 - val_loss: 0.0425 - val_acc: 0.9871\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0405 - acc: 0.9871 - val_loss: 0.0339 - val_acc: 0.9892\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 43247s 721ms/step - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0456 - val_acc: 0.9851\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 17850s 297ms/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0444 - val_acc: 0.9869\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0577 - val_acc: 0.9848\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 83s 1ms/step - loss: 0.0112 - acc: 0.9960 - val_loss: 0.0424 - val_acc: 0.9889\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0084 - acc: 0.9971 - val_loss: 0.0464 - val_acc: 0.9888\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0471 - val_acc: 0.9884\n"
     ]
    }
   ],
   "source": [
    "model = build_network('he_uniform')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, train_labels,\n",
    "                    batch_size=64,\n",
    "                    epochs=8,\n",
    "                    validation_data=(x_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly the same -- still learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
